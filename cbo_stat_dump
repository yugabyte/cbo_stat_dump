#!/usr/bin/env python3
'''
cbo_stat_dump utility

Exports statistics and other data needed to reproduce query plan
'''
import argparse
import json
import os
import re
import shutil
import subprocess
import sys
import time
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
sys.path.append(os.path.dirname(SCRIPT_DIR))
from cbo_stat_dump import __version__
from _ctypes import PyObj_FromPtr
from pathlib import Path
import psycopg2
import urllib.request, json
import logging

logger = logging.getLogger(__name__)

PG_DUMP_BIN = 'pg_dump'
YSQL_DUMP_BIN = 'ysql_dump'

QUERY_FILE_NAME = 'query.sql'
DDL_FILE_NAME = 'ddl.sql'
QUERY_PLAN_FILE_NAME = 'query_plan.txt'
STATISTICS_JSON_FILE_NAME = 'statistics.json'
STATISTIC_EXT_JSON_FILE_NAME = 'statistic_ext.json'
IMPORT_STATISTICS_SQL_FILE_NAME = 'import_statistics.sql'
IMPORT_STATISTIC_EXT_SQL_FILE_NAME = 'import_statistics_ext.sql'
OVERRIDDEN_GUCS_FILE_NAME = 'overridden_gucs.sql'
GFLAGS_FILE_NAME = 'gflags.json'
VERSION_FILE_NAME = 'version.txt'
PG_CLASS_FILE_NAME = 'pg_class.json'
DEFAULT_OUT_DIR_PREFIX = 'query_planner_data_'

CBO_RELEVANT_GUC_PARAMS = {'enable_seqscan',
                           'enable_indexscan',
                           'enable_bitmapscan',
                           'enable_indexonlyscan',
                           'enable_bitmapscan',
                           'enable_tidscan',
                           'enable_sort',
                           'enable_hashagg',
                           'enable_nestloop',
                           'enable_material',
                           'enable_mergejoin',
                           'enable_hashjoin',
                           'enable_gathermerge',
                           'enable_partitionwise_join',
                           'enable_partitionwise_aggregate',
                           'enable_parallel_append',
                           'enable_parallel_hash',
                           'enable_partition_pruning',
                           'random_page_cost',
                           'seq_page_cost',
                           'cpu_tuple_cost',
                           'cpu_index_tuple_cost',
                           'cpu_operator_cost',
                           'effective_cache_size',
                           'shared_buffers',
                           'work_mem',
                           'maintenance_work_mem',
                           'default_statistics_target',
                           'max_parallel_workers_per_gather'}

YB_CBO_RELEVANT_GUC_PARAMS = {'yb_enable_geolocation_costing',
                              'yb_enable_batchednl',
                              'yb_enable_parallel_append',
                              'yb_enable_bitmapscan',
                              'yb_enable_base_scans_cost_model',
                              'yb_bnl_batch_size',
                              'yb_enable_expression_pushdown',
                              'yb_enable_geolocation_costing',
                              'yb_test_planner_custom_plan_threshold'}

STATISTICS_FETCH_ROWS_SIZE = 1000


def get_connection_dict(args):
    host = args.host
    port = args.port
    username = args.username
    password = args.password
    db = args.database
    logger.debug(f"Connecting to database host={host}, port={port}, username={username}, db={db}")

    return {
        'host': host,
        'port': port,
        'user': username,
        'password': password,
        'database': db
    }


def connect_database(connection_dict):
    conn = psycopg2.connect(**connection_dict)
    cursor = conn.cursor()
    return conn, cursor


def parse_cmd_line():
    parser = argparse.ArgumentParser(
        prog='cbo_stat_dump',
        description='Exports statistics and other data to reproduce query plan',
        add_help=False
    )

    parser.add_argument('--help', action='store_true', help='show this help message and exit')
    parser.add_argument('--debug', action='store_true', help='Set log level to DEBUG')
    parser.add_argument('-h', '--host', help='Hostname or IP address, default localhost',
                            default="localhost")
    parser.add_argument('-p', '--port', help='Port number')
    parser.add_argument('-d', '--database', help='Database name')
    parser.add_argument('-s', '--schemas', type=lambda x: x.split(','),
                        help='Comma separated list of schema names. Use quotes to include spaces in schema names, default all schemas apart from pg_catalog, pg_toast, and information_schema')
    parser.add_argument('-u', '--username', help='Username')
    parser.add_argument('-W', '--password', help='Password, default no password')
    parser.add_argument('-o', '--out_dir',
                        default='/tmp/' + DEFAULT_OUT_DIR_PREFIX + time.strftime("%Y%m%d-%H%M%S"),
                        help='Output directory')
    parser.add_argument('-q', '--query_file',
                        help='File containing query that needs to be debugged')
    parser.add_argument('--yb_mode', action='store_true',
                        help='Use this mode to export data from YugabyteDB')
    parser.add_argument('--enable_base_scans_cost_model', action=argparse.BooleanOptionalAction,
                        help='Set yb_enable_base_scans_cost_model=ON before running explain on query')

    args = parser.parse_args()

    if args.help:
        parser.print_help()
        sys.exit(0)

    logging.basicConfig(level=logging.INFO)
    if args.debug:
        logging.basicConfig(level=logging.DEBUG)

    if not args.yb_mode and args.enable_base_scans_cost_model:
        logger.fatal(
            '\n--enable_base_scans_cost_model can only be used with --yb_mode\n\n')
        sys.exit(1)

    if args.schemas and args.query_file:
        logger.fatal(
            '\nIncompatible options: --schemas and --query_file are mutually exclusive\n\n')
        sys.exit(1)

    if args.yb_mode:
        if args.username is None:
            args.username = 'yugabyte'
        if args.database is None:
            args.database = 'yugabyte'
        if args.port is None:
            args.port = 5433
        global CBO_RELEVANT_GUC_PARAMS
        CBO_RELEVANT_GUC_PARAMS.update(YB_CBO_RELEVANT_GUC_PARAMS)
    else:
        if args.username is None:
            args.username = 'postgres'
        if args.database is None:
            args.database = 'postgres'
        if args.port is None:
            args.port = 5432
    return args


def get_relations_from_json_recurse(query_plan_json):
    relations = []
    if 'Plans' in query_plan_json:
        for sub_plan_json in query_plan_json['Plans']:
            if 'Relation Name' in sub_plan_json:
                assert('Schema' in sub_plan_json)
                relations.append(sub_plan_json['Schema'] + '.' + sub_plan_json['Relation Name'])
            relations = relations + (get_relations_from_json_recurse(sub_plan_json))
    return relations


def get_relation_names_in_query(cursor, query_file):
    with open(query_file, 'r') as sql_f:
        sql_text = sql_f.read()
    explain_query = f"EXPLAIN (FORMAT JSON, VERBOSE) {sql_text}"

    try:
        cursor.execute(explain_query)
        query_plan_json = cursor.fetchone()[0][0]
    except Exception as e:
        logger.debug(f"Failed to evaluate:\n{explain_query}")
        raise e

    relations = []
    if 'Relation Name' in query_plan_json['Plan']:
        assert('Schema' in query_plan_json['Plan'])
        relations.append(query_plan_json['Plan']['Schema'] + '.' + query_plan_json['Plan']['Relation Name'])
    relations.extend(get_relations_from_json_recurse(query_plan_json['Plan']))
    relations = list(dict.fromkeys(relations))
    return relations


def get_relation_oids(cursor, relation_names):
    relation_oids = []
    for relation_name in relation_names:
        oid_query = f"SELECT oid FROM pg_class WHERE relname='{relation_name}'"
        cursor.execute(oid_query)
        relation_oids.append(cursor.fetchone()[0])
    return relation_oids


def get_process_output(cmd):
    p = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE,
                         env=os.environ)
    outmsg, errormsg = p.communicate()
    if p.returncode != 0:
        logger.fatal(
            f'\nError when executing the following command.\n\n{cmd}\n{errormsg.decode()}\n\n')
        sys.exit(1)
    return outmsg.decode('UTF-8')


def extract_ddl(connection_dict, schemas, relation_names, output_dir, yb_mode):
    DDL_DUMP_BIN = PG_DUMP_BIN
    if yb_mode:
        DDL_DUMP_BIN = YSQL_DUMP_BIN
    if not shutil.which(DDL_DUMP_BIN):
        logger.fatal(f"{DDL_DUMP_BIN} binary not found.\n\n")
        sys.exit(1)

    ddl_file_name = f'{output_dir}/{DDL_FILE_NAME}'
    logger.debug(f"Exporting DDL to {ddl_file_name}")
    ddl_tmp_file_name = ddl_file_name + '.tmp'
    connection_params = f'-d {connection_dict["database"]} -h {connection_dict["host"]} -p {connection_dict["port"]} -U {connection_dict["user"]}'

    password_prefix_str = ""
    if connection_dict["password"] is not None:
        password_prefix_str = f'PGPASSWORD={connection_dict["password"]} '
    if relation_names:
        ddl_dump_cmd = password_prefix_str + (f"{DDL_DUMP_BIN} {connection_params} -s")
        for relation_name in relation_names:
            ddl_dump_cmd = ddl_dump_cmd + f" -t {relation_name}"

        outmsg = get_process_output(ddl_dump_cmd)
        with open(ddl_tmp_file_name, 'a') as ddl_tmp_file:
            ddl_tmp_file.write(outmsg)
    else:
        ddl_dump_cmd = password_prefix_str + f"{DDL_DUMP_BIN} {connection_params} -s"
        if schemas:
            schemas_str = ''.join([f' -n {schema.strip()}' for schema in schemas])
            ddl_dump_cmd += schemas_str
        outmsg = get_process_output(ddl_dump_cmd)
        with open(ddl_tmp_file_name, 'w') as ddl_tmp_file:
            ddl_tmp_file.write(outmsg)
    with open(ddl_file_name, 'w') as ddl_file:
        with open(ddl_tmp_file_name, 'r') as ddl_tmp_file:
            for line in ddl_tmp_file:
                if line.strip():
                    m = re.match("(?:^--)|(?:^SET)|(?:^SELECT pg_catalog)|(?:^ALTER .+ OWNER TO)|(?:^CREATE SCHEMA public;$)", line)

                    if m is None:
                        ddl_file.write(line)
        ddl_file.write('\n')
    os.remove(ddl_tmp_file_name)


def export_query_file(query_file, out_dir):
    query_file_name = f'{out_dir}/{QUERY_FILE_NAME}'
    logger.debug(f"Exporting query file to {query_file_name}")
    shutil.copy(query_file, query_file_name)


def export_query_plan(cursor, query_file, out_dir, enable_base_scans_cost_model):
    query_plan_file_name = f'{out_dir}/{QUERY_PLAN_FILE_NAME}'
    logger.debug(f"Exporting query plan to {query_plan_file_name}")
    with open(query_file, 'r') as sql_f:
        sql_text = sql_f.read()

    if enable_base_scans_cost_model:
        cursor.execute('SET yb_enable_base_scans_cost_model=ON')
    explain_query = f"EXPLAIN {sql_text}"
    cursor.execute(explain_query)
    query_plan = cursor.fetchall()
    with open(query_plan_file_name, 'w') as query_plan_file:
        for tuple in query_plan:
            query_plan_file.write(tuple[0] + '\n')


# Custom JSON encoder to improve readability of statistics.json file
# Derived from https://stackoverflow.com/a/13252112
class NoIndent(object):
    """ Value wrapper. """

    def __init__(self, value):
        self.value = value


class CustomIndentEncoder(json.JSONEncoder):
    FORMAT_SPEC = '@@{}@@'
    regex = re.compile(FORMAT_SPEC.format(r'(\d+)'))

    def __init__(self, **kwargs):
        # Save copy of any keyword argument values needed for use here.
        self.__sort_keys = kwargs.get('sort_keys')
        super(CustomIndentEncoder, self).__init__(**kwargs)

    def default(self, obj):
        return (self.FORMAT_SPEC.format(id(obj)) if isinstance(obj, NoIndent)
                else super(CustomIndentEncoder, self).default(obj))

    def encode(self, obj):
        format_spec = self.FORMAT_SPEC   # Local var to expedite access.
        json_repr = super(CustomIndentEncoder, self).encode(obj)  # Default JSON.

        # Replace any marked-up object ids in the JSON repr with the
        # value returned from the json.dumps() of the corresponding
        # wrapped Python object.
        for match in self.regex.finditer(json_repr):
            id = int(match.group(1))
            no_indent = PyObj_FromPtr(id)
            json_obj_repr = json.dumps(no_indent.value, sort_keys=self.__sort_keys)

            # Replace the matched id string with json formatted representation
            # of the corresponding Python object.
            json_repr = json_repr.replace(f'"{format_spec.format(id)}"', json_obj_repr)

        return json_repr

def get_pg_statatistic_insert_query(stat_json):
    columnTypes = { "stainherit": "boolean",
                    "stanullfrac": "real",
                    "stawidth": "integer",
                    "stadistinct": "real",
                    "stakind1": "smallint",
                    "stakind2": "smallint",
                    "stakind3": "smallint",
                    "stakind4": "smallint",
                    "stakind5": "smallint",
                    "staop1": "oid",
                    "staop2": "oid",
                    "staop3": "oid",
                    "staop4": "oid",
                    "staop5": "oid",
                    "stacoll1": "oid",
                    "stacoll2": "oid",
                    "stacoll3": "oid",
                    "stacoll4": "oid",
                    "stacoll5": "oid",
                    "stanumbers1": "real[]",
                    "stanumbers2": "real[]",
                    "stanumbers3": "real[]",
                    "stanumbers4": "real[]",
                    "stanumbers5": "real[]"}

    if 'typnspname' not in stat_json:
        stavaluesType = 'pg_catalog'
    else:
        stavaluesType = stat_json['typnspname']

    stavaluesType = stavaluesType + '.' + stat_json["typname"]
    for i in range (1, 5):
        columnTypes["stavalues" + str(i)] = stavaluesType

    columnValues = ""
    for columnName, columnType in columnTypes.items():
        val = stat_json[columnName]
        sql_val = ""
        if (columnName.startswith('stavalues')):
            # Convert a python list into SQL array string representation '{"...", "..."}'
            sql_array = ""
            if val is None:
                sql_val = "NULL"
            else:
                if isinstance(val[0], str):
                    # Escape backslash and double quotes with backslash, but single quote with single quote
                    val = [f'"%s"' % e.replace('\\', '\\\\').replace('"', '\\"').replace('\'', '\'\'') for e in val]
                    sql_array = ', '.join(val)
                elif isinstance(val[0], dict):
                    # Array of JSON Objects, the type of the column must be jsonb
                    assert(stavaluesType == 'pg_catalog.jsonb')
                    val = [f'"%s"' % str(e).replace('"', '\\\\\\\"').replace("'", '\\"').replace('True', 'true').replace('False', 'false') for e in val]
                    sql_array = ', '.join(val)
                else:
                    val = [f"%s" % e for e in val]
                    sql_array = ', '.join(val)
                sql_array = f"'{{{sql_array}}}'"
                sql_val = f"array_in({sql_array}, '{columnType}'::regtype, -1)::anyarray"
        elif isinstance(val, list):
            assert(columnType == 'real[]')
            sql_val = str(val);
            sql_val = f"'{{{sql_val[1:-1]}}}'::{columnType}"
        elif val is None:
            sql_val = 'NULL::' + columnType
        else:
            sql_val = str(stat_json[columnName]) + "::" + columnType
        columnValues += ", " + sql_val

    starelid = f"'{stat_json['nspname']}.{stat_json['relname']}'::regclass"
    staattnum_subquery = f"(SELECT a.attnum FROM pg_attribute a WHERE a.attrelid = {starelid} and a.attname = '{stat_json['attname']}')"
    query = f"""
DELETE FROM pg_statistic WHERE starelid = {starelid} AND staattnum = {staattnum_subquery};
INSERT INTO pg_statistic VALUES ({starelid}, {staattnum_subquery}{columnValues}); \
"""
    return query


def export_extended_statistics(args, cursor, schemas, relation_names, out_dir):
    statistic_ext_json_file_name = f'{out_dir}/{STATISTIC_EXT_JSON_FILE_NAME}'
    import_statistic_ext_sql_file_name = f'{out_dir}/{IMPORT_STATISTIC_EXT_SQL_FILE_NAME}'

    logger.debug(f"Exporting data from pg_statistic_ext and pg_statistic_ext_data to file {statistic_ext_json_file_name}")
    statistic_ext_dict = {}
    
    statistic_ext_dict['version'] = __version__
    
    schemas_filter = ""
    if schemas:
        schemas_str = ', '.join([f"\'{schema.strip()}\'" for schema in schemas])
        schemas_filter = f" and n.nspname in ({schemas_str}) "
    else:
        schemas_filter = " and n.nspname not in ('pg_catalog', 'pg_toast', 'information_schema')"

    relation_names_filter = ""
    if relation_names:
        relation_names_str = ', '.join([f"\'{relation_name}\'::regclass::oid" for relation_name in relation_names])
        relation_names_filter = f" and (c.oid in ({relation_names_str}) or c.oid in (select indexrelid from pg_index where indrelid in ({relation_names_str}))) "
        
    query = f"""
        SELECT row_to_json(t) FROM 
            (SELECT c.relname, s.stxname, n.nspname, s.stxowner, s.stxstattarget, string_agg(a.attname, ',') as stxkeys, s.stxkind, s.stxexprs 
             FROM 
                pg_class c 
                JOIN pg_statistic_ext s ON c.oid = s.stxrelid 
                JOIN pg_attribute a ON c.oid = a.attrelid AND a.attnum = ANY(s.stxkeys)
                JOIN pg_namespace n ON c.relnamespace = n.oid {schemas_filter} {relation_names_filter}
                GROUP BY c.relname, s.stxname, n.nspname, s.stxowner, s.stxstattarget, s.stxkind, s.stxexprs) t"""
    cursor.execute(query)
    rows_pg_statistic_ext = cursor.fetchall()
    list_pg_statistic_ext = [NoIndent(row[0]) for row in rows_pg_statistic_ext]
    statistic_ext_dict['pg_statistic_ext'] = list_pg_statistic_ext
    
    query = f"""
        SELECT row_to_json(t) FROM 
            (SELECT s.stxname, d.stxdinherit, d.stxdndistinct, d.stxddependencies, d.stxdmcv, d.stxdexpr
                FROM
                    pg_statistic_ext s JOIN pg_statistic_ext_data d ON s.oid = d.stxoid) t"""
    cursor.execute(query)
    rows_pg_statistic_ext_data = cursor.fetchall()
    dict_statistic_ext_to_data = {}
    for row in rows_pg_statistic_ext_data:
        dict_statistic_ext_to_data[row[0]['stxname']] = row[0]
    list_pg_statistic_ext_data = [NoIndent(row[0]) for row in rows_pg_statistic_ext_data]
    statistic_ext_dict['pg_statistic_ext_data'] = list_pg_statistic_ext_data
    
    statistic_ext_json_str = json.dumps(statistic_ext_dict, indent=4, cls=CustomIndentEncoder)
    statistic_ext_json = json.loads(statistic_ext_json_str)

    with open(statistic_ext_json_file_name, 'w') as statistics_json_file:
        statistics_json_file.write(statistic_ext_json_str)

    with open(import_statistic_ext_sql_file_name, 'w') as import_statistics_sql_file:
        if args.yb_mode:
            import_statistics_sql_file.write('SET yb_non_ddl_txn_for_sys_tables_allowed = ON;\n\n')

        for pg_statistic_ext_data_row_json in statistic_ext_json['pg_statistic_ext_data']:
            stxdndistinct = pg_statistic_ext_data_row_json['stxdndistinct'] if pg_statistic_ext_data_row_json['stxdndistinct'] is not None else 'NULL'
            stxddependencies = pg_statistic_ext_data_row_json['stxddependencies'] if pg_statistic_ext_data_row_json['stxddependencies'] is not None else 'NULL'
            stxdmcv = pg_statistic_ext_data_row_json['stxdmcv'] if pg_statistic_ext_data_row_json['stxdmcv'] is not None else 'NULL'
            stxdexpr = pg_statistic_ext_data_row_json['stxdexpr'] if pg_statistic_ext_data_row_json['stxdexpr'] is not None else 'NULL'
            
            query = f"""\
INSERT INTO pg_statistic_ext_data VALUES ((SELECT stxoid FROM pg_statistics_ext WHERE stxname='{pg_statistic_ext_data_row_json['stxname']}'), {pg_statistic_ext_data_row_json['stxdinherit']}, {stxdndistinct}, {stxddependencies}, {stxdmcv}, {stxdexpr});\n"""
            import_statistics_sql_file.write(query)

        if args.yb_mode:
            import_statistics_sql_file.write('\nupdate pg_yb_catalog_version set current_version=current_version+1 where db_oid=1;\n')
            import_statistics_sql_file.write('SET yb_non_ddl_txn_for_sys_tables_allowed = OFF;\n')


def export_statistics(args, cursor, schemas, relation_names, out_dir):
    statistics_json_file_name = f'{out_dir}/{STATISTICS_JSON_FILE_NAME}'
    import_statistics_sql_file_name = f'{out_dir}/{IMPORT_STATISTICS_SQL_FILE_NAME}'

    logger.debug(f"Exporting data from pg_statistic and pg_class to file {statistics_json_file_name}")
    statistics_dict = {}

    statistics_dict['version'] = __version__

    schemas_filter = ""
    if schemas:
        schemas_str = ', '.join([f"\'{schema.strip()}\'" for schema in schemas])
        schemas_filter = f" and n.nspname in ({schemas_str}) "
    else:
        schemas_filter = " and n.nspname not in ('pg_catalog', 'pg_toast', 'information_schema')"

    relation_names_filter = ""
    if relation_names:
        relation_names_str = ', '.join([f"\'{relation_name}\'::regclass::oid" for relation_name in relation_names])
        relation_names_filter = f" and (c.oid in ({relation_names_str}) or c.oid in (select indexrelid from pg_index where indrelid in ({relation_names_str}))) "

    query = f"""
        SELECT row_to_json(t) FROM
            (SELECT c.relname, c.relpages, c.reltuples, c.relallvisible, n.nspname
                FROM pg_class c JOIN pg_namespace n on c.relnamespace = n.oid {schemas_filter} {relation_names_filter}) t
        """
    cursor.execute(query)
    rows_pg_class = cursor.fetchall()
    list_pg_class = [NoIndent(row[0]) for row in rows_pg_class]
    statistics_dict['pg_class'] = list_pg_class
    query = f"""
        SELECT row_to_json(t) FROM
            (SELECT
                n.nspname nspname,
                c.relname relname,
                a.attname attname,
                (select nspname from pg_namespace where oid = t.typnamespace) typnspname,
                t.typname typname,
                s.stainherit,
                s.stanullfrac,
                s.stawidth,
                s.stadistinct,
                s.stakind1,
                s.stakind2,
                s.stakind3,
                s.stakind4,
                s.stakind5,
                s.staop1,
                s.staop2,
                s.staop3,
                s.staop4,
                s.staop5,
                s.stanumbers1,
                s.stanumbers2,
                s.stanumbers3,
                s.stanumbers4,
                s.stanumbers5,
                s.stacoll1,
                s.stacoll2,
                s.stacoll3,
                s.stacoll4,
                s.stacoll5,
                s.stavalues1,
                s.stavalues2,
                s.stavalues3,
                s.stavalues4,
                s.stavalues5
                FROM pg_class c
                    JOIN pg_namespace n on c.relnamespace = n.oid {schemas_filter} {relation_names_filter}
                    JOIN pg_statistic s ON s.starelid = c.oid
                    JOIN pg_attribute a ON c.oid = a.attrelid AND s.staattnum = a.attnum
                    JOIN pg_type t ON a.atttypid = t.oid) t
        """

    cursor.execute(query)
    list_pg_statistic = []
    while True:
        if pg_statistic_rows := cursor.fetchmany(STATISTICS_FETCH_ROWS_SIZE):
            list_pg_statistic.extend(NoIndent(row[0]) for row in pg_statistic_rows)
        else:
            break
    statistics_dict['pg_statistic'] = list_pg_statistic
    statistics_json_str = json.dumps(statistics_dict, indent=4, cls=CustomIndentEncoder)
    statistics_json = json.loads(statistics_json_str)

    with open(statistics_json_file_name, 'w') as statistics_json_file:
        statistics_json_file.write(statistics_json_str)

    with open(import_statistics_sql_file_name, 'w') as import_statistics_sql_file:
        if args.yb_mode:
            import_statistics_sql_file.write('SET yb_non_ddl_txn_for_sys_tables_allowed = ON;\n\n')

        for pg_class_row_json in statistics_json['pg_class']:
            query = f'''\
UPDATE pg_class SET reltuples = {pg_class_row_json["reltuples"]}, \
relpages = {pg_class_row_json["relpages"]}, relallvisible = {pg_class_row_json["relallvisible"]} \
WHERE relnamespace = \'{pg_class_row_json["nspname"]}\'::regnamespace AND \
(relname = \'{pg_class_row_json["relname"]}\' OR relname = '{pg_class_row_json["relname"]}_pkey');
'''
            import_statistics_sql_file.write(query)

        for pg_statistic_row_json in statistics_json['pg_statistic']:
            query = get_pg_statatistic_insert_query(pg_statistic_row_json)
            import_statistics_sql_file.write(query)

        if args.yb_mode:
            import_statistics_sql_file.write('\nupdate pg_yb_catalog_version set current_version=current_version+1 where db_oid=1;\n')
            import_statistics_sql_file.write('SET yb_non_ddl_txn_for_sys_tables_allowed = OFF;\n')


def export_overridden_gucs(cursor, out_dir):
    overridden_gucs_file_name = f'{out_dir}/{OVERRIDDEN_GUCS_FILE_NAME}'
    logger.debug(f"Exporting overridden GUCs to {overridden_gucs_file_name}")
    overridden_gucs_list = [];
    cursor.execute('SELECT name, setting from pg_settings where setting <> boot_val')
    while True:
        if pg_settings_rows := cursor.fetchmany(STATISTICS_FETCH_ROWS_SIZE):
            for row in pg_settings_rows:
                if row[0] in CBO_RELEVANT_GUC_PARAMS:
                    overridden_gucs_list.append(f'SET {row[0]}=\'{row[1]}\';\n')
        else:
            break

    with open(overridden_gucs_file_name, 'w') as overridden_gucs_file:
        for line in overridden_gucs_list:
            overridden_gucs_file.write(line)


def export_gflags(host, out_dir):
    gflags_file_name = f'{out_dir}/{GFLAGS_FILE_NAME}'
    logger.debug(f"Exporting gflags to {gflags_file_name}")
    gflags_dict = {}
    try :
        with urllib.request.urlopen(f'http://{host}:7000/api/v1/varz') as url:
            data = json.load(url)
            for flag in data['flags']:
                if flag['type'] == 'Custom':
                    gflags_dict[flag['name']] = flag['value']

        gflags_json = json.dumps(gflags_dict, indent = 4)
        with open(gflags_file_name, 'w') as gflags_file:
            gflags_file.write(gflags_json)
    except Exception as e:
        logger.debug ("Failed to get gflags.")
        return


def export_version(cursor, out_dir):
    version_file_name = f'{out_dir}/{VERSION_FILE_NAME}'
    logger.debug(f"Exporting version to {version_file_name}")
    cursor.execute("select version()")
    with open(version_file_name, 'w') as version_file:
        version_file.write(cursor.fetchone()[0])


def set_extra_float_digits(cursor, digits):
    cursor.execute(f'SET extra_float_digits = {digits}')


def main():
    args = parse_cmd_line()
    connection_dict = get_connection_dict(args)
    conn, cursor = connect_database(connection_dict)
    Path(args.out_dir).mkdir(parents=True, exist_ok=True)
    out_dir_abs_path = os.path.abspath(args.out_dir)
    relation_names = []
    if args.query_file is not None:
        relation_names = get_relation_names_in_query(cursor, args.query_file)
        export_query_file(args.query_file, out_dir_abs_path)
        export_query_plan(cursor, args.query_file, out_dir_abs_path, args.enable_base_scans_cost_model)
    set_extra_float_digits(cursor, 3)
    extract_ddl(connection_dict, args.schemas, relation_names, out_dir_abs_path, args.yb_mode)
    export_statistics(args, cursor, args.schemas, relation_names, out_dir_abs_path)
    export_extended_statistics(args, cursor, args.schemas, relation_names, out_dir_abs_path)
    export_version(cursor, out_dir_abs_path)
    export_overridden_gucs(cursor, out_dir_abs_path)
    if args.yb_mode:
        export_gflags(args.host, out_dir_abs_path)
    cursor.close()
    conn.close()


if __name__ == "__main__":
    main()
